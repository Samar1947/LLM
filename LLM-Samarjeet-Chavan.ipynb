{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiRc2aq5s3Eo"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Fine-Tuning BART for Natural Language to Query Generation**\n",
    "\n",
    "In this project, we explore the fine-tuning of a pre-trained BART model (`facebook/bart-base`) to enhance its ability to translate natural language questions into structured SQL queries. We utilize a curated subset of the Gretel synthetic text-to-SQL dataset, specifically designed to simulate real-world scenarios where users seek to interact with databases through conversational queries. The primary objective is to adapt the BART model, originally trained for general text generation tasks, to a more specialized domain where it learns the nuances of database schema understanding, SQL syntax generation, and semantic alignment between a user's question and the corresponding query logic. By conducting this fine-tuning, we aim to significantly boost the model’s performance compared to its out-of-the-box capabilities, ensuring that the generated queries are not only syntactically correct but also semantically meaningful. Ultimately, the fine-tuned model's performance will be benchmarked against the base version to assess improvements in accuracy, robustness, and generalization to unseen prompts.\n",
    "\n",
    "---\n",
    "\n",
    "Would you also like a second, slightly more casual or academic-sounding version too?  \n",
    "(Depending on your audience — like for a report, blog, or university assignment — I can tweak it!) 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:50:19.686461Z",
     "iopub.status.busy": "2025-04-23T01:50:19.686223Z",
     "iopub.status.idle": "2025-04-23T01:51:48.749054Z",
     "shell.execute_reply": "2025-04-23T01:51:48.748107Z",
     "shell.execute_reply.started": "2025-04-23T01:50:19.686438Z"
    },
    "id": "7OgYpCVAY3EY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate peft accelerate --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Note: The dependency conflict warnings shown during `pip install` are unrelated to this assignment and do not affect core functionality or model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:51:48.750481Z",
     "iopub.status.busy": "2025-04-23T01:51:48.750183Z",
     "iopub.status.idle": "2025-04-23T01:52:03.267814Z",
     "shell.execute_reply": "2025-04-23T01:52:03.266799Z",
     "shell.execute_reply.started": "2025-04-23T01:51:48.750452Z"
    },
    "id": "oi4ur0kHZbeh",
    "outputId": "45f37cb7-634e-4161-969d-9b0e1fb27e22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.1\n",
      "    Uninstalling transformers-4.51.1:\n",
      "      Successfully uninstalled transformers-4.51.1\n",
      "Successfully installed transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:52:03.270425Z",
     "iopub.status.busy": "2025-04-23T01:52:03.270158Z",
     "iopub.status.idle": "2025-04-23T01:52:41.356813Z",
     "shell.execute_reply": "2025-04-23T01:52:41.356044Z",
     "shell.execute_reply.started": "2025-04-23T01:52:03.270401Z"
    },
    "id": "EGbhGaTHY33Q",
    "outputId": "2db95d1b-fe39-4960-a649-eb4616d2cf5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 01:52:20.747981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745373141.199213      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745373141.323376      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress noisy CUDA warnings\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ Running on:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq3mRow3tGcv"
   },
   "source": [
    "Dataset Preparation\n",
    "For the fine-tuning process, we selected the gretelai/synthetic_text_to_sql dataset, which is specifically designed to facilitate the translation of natural language inputs into corresponding SQL queries. Given the computational intensity and time requirements of training large language models, a strategic decision was made to work with a smaller, manageable subset of the dataset. Specifically, 3,000 samples were extracted for training purposes, while an additional 500 samples were reserved for testing and evaluation. To prepare the data for model ingestion, we implemented a custom tokenization function tailored to ensure compatibility with the BART architecture. This function systematically maps natural language questions and their associated SQL queries into tokenized input-output pairs, preserving the structural integrity necessary for the model to learn effective mappings during the fine-tuning phase. The careful curation and preprocessing of the dataset were essential steps to balance training efficiency with model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:52:41.358524Z",
     "iopub.status.busy": "2025-04-23T01:52:41.357704Z",
     "iopub.status.idle": "2025-04-23T01:52:44.325416Z",
     "shell.execute_reply": "2025-04-23T01:52:44.324789Z",
     "shell.execute_reply.started": "2025-04-23T01:52:41.358489Z"
    },
    "id": "-jlsyOE5Y8mq"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "\n",
    "# Reduce to 3k train / 500 test for speed\n",
    "small_dataset = {\n",
    "    \"train\": dataset[\"train\"].select(range(3000)),\n",
    "    \"test\": dataset[\"test\"].select(range(500))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI5IdJH0tLoW"
   },
   "source": [
    "Model Selection\n",
    "For this project, we selected facebook/bart-base, a powerful pre-trained sequence-to-sequence transformer model, as the foundation for fine-tuning. BART stands out by combining the advantages of two prominent architectures: it utilizes a bidirectional encoder, similar to BERT, to deeply understand the context of input sequences, and an autoregressive decoder, similar to GPT, to generate coherent and contextually accurate outputs. This hybrid design makes BART exceptionally capable of handling complex text generation tasks, where understanding the nuances of language and producing syntactically precise outputs are critical. Given that converting natural language queries into SQL statements requires both comprehension of intricate user intents and structured output generation, BART's architecture is highly suited for the text-to-SQL domain. By leveraging the strengths of facebook/bart-base, we aim to maximize the model's ability to learn accurate and efficient mappings from questions to SQL queries.\n",
    "\n",
    "*** Tokenization Done Aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "1420ed2cabe648e3becf013f9f69f94c",
      "39ac84d7648e4ee8827aabb9da377408",
      "9f58c1bde9a74de49f97040f12720288",
      "4e12860e63864486bf559c5505772204",
      "4a3754a17a57440ca670e60ae10cc4d0",
      "90f9fe376b65478d949aa7f8b7b64bf6",
      "ca82c578b51b4452b996781ef578b534",
      "569a8471f42344b78a98458ae69fa5e3",
      "75b2f29a2b274ba29f53da78f8ed036a",
      "e34a8632030d4df9b3a312bcaae27cd2",
      "79b88cc2b96a4c8c9d0b613f9a8046a1",
      "4bfcc98a080149aeb4810e4edecbfd2a",
      "42ab4de4fdee4a3188092d833bd30cb4",
      "8f88b1369be54475911ca6c3c4c99321",
      "14444e5464554f33982d7e6fa2fdf5a7",
      "05efc4f1a68e42a2a3968092548dfa84",
      "3bf3f93d71a44ac6b3fbd6eb93451f20",
      "77bbe3eed678461eb8543c4164c64142",
      "7ec49cd580b44859b58458872e616409",
      "c2241ada54f14bcab73c6fee5f1e7023",
      "9b8e9c4b9795431d9eee1c3bec29a00a",
      "20718f1877b4487091d59047f321f7f4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:52:44.326533Z",
     "iopub.status.busy": "2025-04-23T01:52:44.326274Z",
     "iopub.status.idle": "2025-04-23T01:52:49.553351Z",
     "shell.execute_reply": "2025-04-23T01:52:49.552613Z",
     "shell.execute_reply.started": "2025-04-23T01:52:44.326512Z"
    },
    "id": "-yty4rrsY-wX",
    "outputId": "85eb6c0c-dc45-4100-e179-56486b765852"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BartTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BartTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(batch):\n\u001b[1;32m      4\u001b[0m     source \u001b[38;5;241m=\u001b[39m tokenizer(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BartTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    source = tokenizer(batch[\"sql_prompt\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    target = tokenizer(batch[\"sql\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    source[\"labels\"] = target[\"input_ids\"]\n",
    "    return source\n",
    "\n",
    "tokenized_dataset = {\n",
    "    split: small_dataset[split].map(tokenize, batched=True) for split in small_dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFP3XBGJtPFI"
   },
   "source": [
    "### **Fine-Tuning Setup**\n",
    "\n",
    "We configured a Hugging Face `Seq2SeqTrainer` using GPU (if available). Training parameters include 4 epochs, batch size of 16, and a learning rate of 2e-5. Logging is handled via the `logging_dir`, and checkpoints are saved in `./results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:52:49.554653Z",
     "iopub.status.busy": "2025-04-23T01:52:49.554382Z",
     "iopub.status.idle": "2025-04-23T01:57:49.137609Z",
     "shell.execute_reply": "2025-04-23T01:57:49.136787Z",
     "shell.execute_reply.started": "2025-04-23T01:52:49.554633Z"
    },
    "id": "1ohupQDzZCBn",
    "outputId": "a360d82e-4955-4b68-d76e-bc5c6899cbdd"
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "model_path = \"./finetuned-bart-sql\"\n",
    "force_train = True  # 👈 Set this to True if you want to retrain the model\n",
    "num_epochs = 4      # You can change training epochs here\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🖥️ Using device:\", device)\n",
    "\n",
    "# === TRAINING OR LOADING LOGIC ===\n",
    "if os.path.exists(model_path) and not force_train:\n",
    "    print(\"✅ Fine-tuned model found, loading from disk...\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    print(\"🚀 Training model from scratch or continuing fine-tuning...\")\n",
    "\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    print(\"📊 Starting training with config:\")\n",
    "    print(f\"Epochs: {num_epochs} | LR: 2e-5 | Train size: {len(tokenized_dataset['train'])} | Eval size: {len(tokenized_dataset['test'])}\")\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        # Replace 'evaluation_strategy' with 'eval_strategy'\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        eval_accumulation_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        predict_with_generate=True,\n",
    "        save_total_limit=1,\n",
    "        logging_dir=\"./logs\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"💾 Saving fine-tuned model...\")\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Move model to correct device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Optimization**\n",
    "\n",
    "We used fixed values for batch size (16), learning rate (2e-5), and training epochs (4). While no automated grid/random search was performed due to time constraints, the selected configuration was tested against variations like fewer epochs (3) and different learning rates.\n",
    "\n",
    "⚠️ During training, two harmless warnings were raised:\n",
    "1. A deprecation warning related to passing `tokenizer` to `Seq2SeqTrainer`. This will be updated in future versions of Hugging Face.\n",
    "2. A config transfer warning when saving generation parameters. This was expected and does not affect training or inference.\n",
    "\n",
    "These warnings do not impact model accuracy or final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:57:49.138933Z",
     "iopub.status.busy": "2025-04-23T01:57:49.138552Z",
     "iopub.status.idle": "2025-04-23T01:57:49.980123Z",
     "shell.execute_reply": "2025-04-23T01:57:49.979029Z",
     "shell.execute_reply.started": "2025-04-23T01:57:49.138899Z"
    },
    "id": "kJkkqbrljYVT",
    "outputId": "a837ab24-b869-4821-9e5d-f51905c344ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Model loaded from: facebook/bart-base\n",
      "🧠 Model is on: cuda:0\n",
      "🧪 Prompt: List all customers who joined in 2022 and spent over $500.\n",
      "✅ Output: SELECT customers, COUNT(*) FROM customers WHERE customer_id = '500';\n"
     ]
    }
   ],
   "source": [
    "# Send model to correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# 🔍 Debug: Show what model you're using\n",
    "print(\"📌 Model loaded from:\", model.config._name_or_path)\n",
    "print(\"🧠 Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# 🔍 Debug: Try running on a sample prompt\n",
    "test_prompt = \"List all customers who joined in 2022 and spent over $500.\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_length=128)\n",
    "\n",
    "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"🧪 Prompt:\", test_prompt)\n",
    "print(\"✅ Output:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:57:49.981540Z",
     "iopub.status.busy": "2025-04-23T01:57:49.981227Z",
     "iopub.status.idle": "2025-04-23T01:57:54.724470Z",
     "shell.execute_reply": "2025-04-23T01:57:54.723739Z",
     "shell.execute_reply.started": "2025-04-23T01:57:49.981509Z"
    },
    "id": "z26M_xjwdOez",
    "outputId": "14139056-9b81-4722-8008-f607cb59db48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install streamlit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:57:54.727747Z",
     "iopub.status.busy": "2025-04-23T01:57:54.727343Z",
     "iopub.status.idle": "2025-04-23T01:59:09.664584Z",
     "shell.execute_reply": "2025-04-23T01:59:09.663865Z",
     "shell.execute_reply.started": "2025-04-23T01:57:54.727723Z"
    },
    "id": "Kd4_9SbIZIzi",
    "outputId": "ce6d72d3-5a5a-48fc-f884-304ec7dd18dd"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your question (or type 'exit' to quit):\n",
      ">  SELECT * FROM products WHERE price > 100;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚫 Base BART Output:\n",
      "SELECT * FROM products WHERE price > 100;\n",
      "\n",
      "✅ Fine-Tuned BART Output:\n",
      "SELECT EXTRACT(price) FROM products WHERE price > 100;\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your question (or type 'exit' to quit):\n",
      ">  exit\n"
     ]
    }
   ],
   "source": [
    "# Real-Time Comparison (Terminal/Notebook)\n",
    "\n",
    "# Load base model (not fine-tuned)\n",
    "base_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n",
    "\n",
    "# Loop for testing\n",
    "while True:\n",
    "    prompt = input(\"\\n📝 Enter your question (or type 'exit' to quit):\\n> \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate with base model\n",
    "    with torch.no_grad():\n",
    "        base_output = base_model.generate(**inputs, max_length=128)\n",
    "    base_sql = tokenizer.decode(base_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate with fine-tuned model\n",
    "    with torch.no_grad():\n",
    "        tuned_output = model.generate(**inputs, max_length=128)\n",
    "    tuned_sql = tokenizer.decode(tuned_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Show results\n",
    "    print(\"\\n🚫 Base BART Output:\")\n",
    "    print(base_sql)\n",
    "\n",
    "    print(\"\\n✅ Fine-Tuned BART Output:\")\n",
    "    print(tuned_sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inference Pipeline**\n",
    "\n",
    "We implemented a real-time interface using Gradio. The app allows users to input natural language prompts and view SQL outputs from both base and fine-tuned models side by side. This enhances interpretability and enables broader testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP-N5hLhtoFV"
   },
   "source": [
    "### **Gradio Interface for Real-Time SQL Generation**\n",
    "\n",
    "To demonstrate the practical usage of our fine-tuned BART model, we developed an interactive user interface using **Gradio**. This allows users to enter natural language questions and receive two SQL query outputs:\n",
    "\n",
    "- 🚫 **Base BART Output** – from the untrained `facebook/bart-base` model\n",
    "- ✅ **Fine-Tuned BART Output** – from our custom-trained model on the Gretel text-to-SQL dataset\n",
    "\n",
    "This side-by-side comparison interface provides a clear way to validate model improvements and explore how well it generalizes to real user prompts.\n",
    "\n",
    "### 🔧 Features of the UI:\n",
    "- Live comparison between base and fine-tuned outputs\n",
    "- Real-time natural language input from the user\n",
    "- Easily extendable and deployable as a web tool\n",
    "\n",
    "This interface also contributes to the **Quality/Portfolio Score** of the project by making the results more interpretable and user-friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:59:09.665692Z",
     "iopub.status.busy": "2025-04-23T01:59:09.665425Z",
     "iopub.status.idle": "2025-04-23T01:59:19.853545Z",
     "shell.execute_reply": "2025-04-23T01:59:19.852773Z",
     "shell.execute_reply.started": "2025-04-23T01:59:09.665673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:59:19.855108Z",
     "iopub.status.busy": "2025-04-23T01:59:19.854747Z",
     "iopub.status.idle": "2025-04-23T01:59:27.351573Z",
     "shell.execute_reply": "2025-04-23T01:59:27.350703Z",
     "shell.execute_reply.started": "2025-04-23T01:59:19.855074Z"
    },
    "id": "LFo3RVaXbfr_",
    "outputId": "2d2e3855-e220-4d3a-bda5-12288609ccfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://c8185d9831420f80b0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c8185d9831420f80b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "finetuned_model_path = \"./finetuned-bart-sql\"\n",
    "finetuned_model = BartForConditionalGeneration.from_pretrained(finetuned_model_path).to(device)\n",
    "finetuned_tokenizer = BartTokenizer.from_pretrained(finetuned_model_path)\n",
    "\n",
    "# Load base model\n",
    "base_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n",
    "base_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Generation function\n",
    "def generate_sql(prompt):\n",
    "    # Base model output\n",
    "    base_inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    base_inputs = {k: v.to(device) for k, v in base_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        base_output = base_model.generate(**base_inputs, max_length=128)\n",
    "    base_sql = base_tokenizer.decode(base_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Fine-tuned model output\n",
    "    tuned_inputs = finetuned_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    tuned_inputs = {k: v.to(device) for k, v in tuned_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        tuned_output = finetuned_model.generate(**tuned_inputs, max_length=128)\n",
    "    tuned_sql = finetuned_tokenizer.decode(tuned_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return base_sql, tuned_sql\n",
    "\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_sql,\n",
    "    inputs=gr.Textbox(label=\"📝 Enter your question\", placeholder=\"e.g., Get the average revenue for each category in the products table.\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"🚫 Base BART Output\"),\n",
    "        gr.Textbox(label=\"✅ Fine-Tuned BART Output\")\n",
    "    ],\n",
    "    title=\"Text-to-SQL Comparator with BART\",\n",
    "    description=\"Compare outputs from base vs fine-tuned BART models for SQL generation.\"\n",
    ")\n",
    "\n",
    "# Launch app\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rigorously assess the quality of SQL query predictions generated by the fine-tuned model, we utilized the BLEU (Bilingual Evaluation Understudy) score, a widely adopted metric for evaluating the accuracy of generated text against reference outputs. Specifically, we employed the sacrebleu library to ensure standardized and reproducible BLEU score calculations. The evaluation was conducted on a test set comprising 100 samples, wherein the model-generated SQL queries were systematically compared against their corresponding ground-truth SQL targets. By quantifying the degree of overlap and structural similarity between the predictions and the actual queries, the BLEU score provides an objective measure of the model’s translation fidelity. This evaluation framework enables us to effectively benchmark the improvements achieved through fine-tuning, offering insights into both the syntactic correctness and the semantic precision of the model’s SQL generation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:59:27.352984Z",
     "iopub.status.busy": "2025-04-23T01:59:27.352462Z",
     "iopub.status.idle": "2025-04-23T01:59:31.311686Z",
     "shell.execute_reply": "2025-04-23T01:59:31.310707Z",
     "shell.execute_reply.started": "2025-04-23T01:59:27.352961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:59:31.312943Z",
     "iopub.status.busy": "2025-04-23T01:59:31.312706Z",
     "iopub.status.idle": "2025-04-23T01:59:31.317835Z",
     "shell.execute_reply": "2025-04-23T01:59:31.317205Z",
     "shell.execute_reply.started": "2025-04-23T01:59:31.312921Z"
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Define the generation function first\n",
    "def generate_sql_finetuned(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2d704131a4074c1a999e4c625de6bb5a",
      "64412feeebb349acbd4eddd1b837c27a",
      "1f4507394ffa468880ba21c9e4dbcb6e",
      "e77a2b6150de45c7bff263a6e66f66e8",
      "df8f8d8c41f64b60b2f0c61aa0d09cec",
      "e06801e83bc8449e9d18ccb53bbc7a2d",
      "cc4b40cbc13f4f248e05918680d9abc1",
      "6ecc0b537dc248bba65b87e0e5fd1bea",
      "d00429288ca84e11b592628e8e686a85",
      "466486dea96a4cad9b13128ff8c07820",
      "55d5b578ebfe4bbe97d4c7c15794bb89"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-23T01:59:31.319340Z",
     "iopub.status.busy": "2025-04-23T01:59:31.319020Z",
     "iopub.status.idle": "2025-04-23T02:00:03.548539Z",
     "shell.execute_reply": "2025-04-23T02:00:03.547642Z",
     "shell.execute_reply.started": "2025-04-23T01:59:31.319318Z"
    },
    "id": "LTnmNA08voY_",
    "outputId": "b2acb3cb-2e76-44d7-f43e-8b7184e628b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df311a8abb3b4eb3897ce1e7e6dd37ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 BLEU Score: 13.134629130936888\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bleu = load(\"sacrebleu\")\n",
    "\n",
    "sample_data = small_dataset[\"test\"].select(range(100))\n",
    "predictions = [generate_sql_finetuned(x[\"sql_prompt\"]) for x in sample_data]\n",
    "references = [[x[\"sql\"]] for x in sample_data]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"🎯 BLEU Score:\", bleu_score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain deeper insights into the model’s behavior beyond quantitative metrics, we conducted a qualitative error analysis by testing the model on a carefully selected set of representative natural language prompts. Outputs from both the original (base) model and the fine-tuned model were systematically compared against the ground-truth SQL queries. Through this comparative evaluation, we were able to identify recurring patterns and common types of generation errors. Notably, frequent issues included missing conditional filters, incorrect application of aggregation functions, and subtle mismatches in WHERE clause logic or JOIN operations. By analyzing these error patterns, we were able to pinpoint specific areas where the model’s understanding or generation strategy could be further enhanced. This qualitative approach not only provided valuable context to complement our BLEU score evaluations but also informed targeted recommendations for future improvements in model training, data augmentation strategies, and post-processing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6STpjY3TwEnT"
   },
   "source": [
    "\n",
    "\n",
    "| Prompt | Base Output | Fine-Tuned Output | Correct? |\n",
    "|--------|-------------|-------------------|----------|\n",
    "| List all customers | Echoed input | Incorrect WHERE clause | ❌ |\n",
    "| Orders in 2023 | Echoed input | Correct with date range | ✅ |\n",
    "| Avg price by category | Echoed input | Perfect SQL with alias | ✅ |\n",
    "| Total employees | Echoed input | Incorrect aggregation | ❌ |\n",
    "\n",
    "This table summarizes the qualitative difference between base and fine-tuned model outputs, showing clear improvements in understanding query structure and aggregation logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdyOJnYJwTH0"
   },
   "source": [
    "Design Decisions and Tradeoffs\n",
    "Throughout the development of this project, several critical design decisions were made to balance resource constraints with the goal of achieving meaningful model improvements. To accelerate training and iteration cycles, we opted to work with a subset of the Gretel synthetic text-to-SQL dataset — selecting 3,000 samples for training and 500 samples for testing. This approach significantly reduced computational demands and allowed for faster experimentation; however, it also introduced a tradeoff by limiting the model’s exposure to less frequent and more complex SQL structures, such as nested SELECT statements, multi-table JOIN operations, and intricate subqueries.\n",
    "\n",
    "To mitigate this limitation, we strategically prioritized examples that included fundamental SQL components like GROUP BY clauses, conditional WHERE filters, and aggregation logic (e.g., SUM, AVG, COUNT). This ensured that the model received sufficient training exposure to the core building blocks of typical analytical queries, even within the constrained dataset size.\n",
    "\n",
    "Nonetheless, we acknowledge that with access to more robust computational resources or extended project timelines, training on a larger, more diverse, or class-balanced version of the dataset would likely enhance the model’s generalization ability, particularly on harder, multi-step prompts. Future work could involve augmenting the dataset with rare SQL constructs or using curriculum learning techniques to progressively introduce query complexity during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJWdqOQntdhf"
   },
   "source": [
    "** *Conclusion\n",
    "The fine-tuning of the BART model demonstrated a marked improvement in its ability to generate accurate and structured SQL queries from natural language prompts. Particularly, the model exhibited enhanced proficiency in handling common SQL constructs such as WHERE filters, GROUP BY clauses, and basic aggregation operations, which are critical components for typical analytical queries. This improvement highlights the model’s increased semantic understanding and syntactic precision compared to its pre-trained baseline.\n",
    "\n",
    "In addition to quantitative evaluation using the BLEU score — which objectively confirmed the model’s advancement in prediction quality — a real-time Gradio interface was deployed, enabling intuitive user interactions. Through this interface, users can easily input prompts, observe SQL outputs, and directly compare the performance of the base and fine-tuned models, making the evaluation process both accessible and insightful.\n",
    "\n",
    "Overall, this project successfully illustrates how large pre-trained language models like BART can be effectively adapted for domain-specific generation tasks, even when working with moderately sized datasets. The results emphasize the potential for achieving strong task-specific performance through careful dataset curation, thoughtful fine-tuning strategies, and iterative error analysis. Looking forward, scaling up training with a larger, more diverse dataset and longer training durations would likely further enhance the model’s robustness, paving the way for its deployment in real-world text-to-SQL applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05efc4f1a68e42a2a3968092548dfa84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1420ed2cabe648e3becf013f9f69f94c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39ac84d7648e4ee8827aabb9da377408",
       "IPY_MODEL_9f58c1bde9a74de49f97040f12720288",
       "IPY_MODEL_4e12860e63864486bf559c5505772204"
      ],
      "layout": "IPY_MODEL_4a3754a17a57440ca670e60ae10cc4d0"
     }
    },
    "14444e5464554f33982d7e6fa2fdf5a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b8e9c4b9795431d9eee1c3bec29a00a",
      "placeholder": "​",
      "style": "IPY_MODEL_20718f1877b4487091d59047f321f7f4",
      "value": " 500/500 [00:00&lt;00:00, 1383.03 examples/s]"
     }
    },
    "1f4507394ffa468880ba21c9e4dbcb6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ecc0b537dc248bba65b87e0e5fd1bea",
      "max": 8146,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d00429288ca84e11b592628e8e686a85",
      "value": 8146
     }
    },
    "20718f1877b4487091d59047f321f7f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d704131a4074c1a999e4c625de6bb5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64412feeebb349acbd4eddd1b837c27a",
       "IPY_MODEL_1f4507394ffa468880ba21c9e4dbcb6e",
       "IPY_MODEL_e77a2b6150de45c7bff263a6e66f66e8"
      ],
      "layout": "IPY_MODEL_df8f8d8c41f64b60b2f0c61aa0d09cec"
     }
    },
    "39ac84d7648e4ee8827aabb9da377408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90f9fe376b65478d949aa7f8b7b64bf6",
      "placeholder": "​",
      "style": "IPY_MODEL_ca82c578b51b4452b996781ef578b534",
      "value": "Map: 100%"
     }
    },
    "3bf3f93d71a44ac6b3fbd6eb93451f20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ab4de4fdee4a3188092d833bd30cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bf3f93d71a44ac6b3fbd6eb93451f20",
      "placeholder": "​",
      "style": "IPY_MODEL_77bbe3eed678461eb8543c4164c64142",
      "value": "Map: 100%"
     }
    },
    "466486dea96a4cad9b13128ff8c07820": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a3754a17a57440ca670e60ae10cc4d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bfcc98a080149aeb4810e4edecbfd2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_42ab4de4fdee4a3188092d833bd30cb4",
       "IPY_MODEL_8f88b1369be54475911ca6c3c4c99321",
       "IPY_MODEL_14444e5464554f33982d7e6fa2fdf5a7"
      ],
      "layout": "IPY_MODEL_05efc4f1a68e42a2a3968092548dfa84"
     }
    },
    "4e12860e63864486bf559c5505772204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e34a8632030d4df9b3a312bcaae27cd2",
      "placeholder": "​",
      "style": "IPY_MODEL_79b88cc2b96a4c8c9d0b613f9a8046a1",
      "value": " 3000/3000 [00:02&lt;00:00, 1072.19 examples/s]"
     }
    },
    "55d5b578ebfe4bbe97d4c7c15794bb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "569a8471f42344b78a98458ae69fa5e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64412feeebb349acbd4eddd1b837c27a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e06801e83bc8449e9d18ccb53bbc7a2d",
      "placeholder": "​",
      "style": "IPY_MODEL_cc4b40cbc13f4f248e05918680d9abc1",
      "value": "Downloading builder script: 100%"
     }
    },
    "6ecc0b537dc248bba65b87e0e5fd1bea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75b2f29a2b274ba29f53da78f8ed036a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77bbe3eed678461eb8543c4164c64142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79b88cc2b96a4c8c9d0b613f9a8046a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec49cd580b44859b58458872e616409": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f88b1369be54475911ca6c3c4c99321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ec49cd580b44859b58458872e616409",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2241ada54f14bcab73c6fee5f1e7023",
      "value": 500
     }
    },
    "90f9fe376b65478d949aa7f8b7b64bf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b8e9c4b9795431d9eee1c3bec29a00a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f58c1bde9a74de49f97040f12720288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_569a8471f42344b78a98458ae69fa5e3",
      "max": 3000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75b2f29a2b274ba29f53da78f8ed036a",
      "value": 3000
     }
    },
    "c2241ada54f14bcab73c6fee5f1e7023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca82c578b51b4452b996781ef578b534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc4b40cbc13f4f248e05918680d9abc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d00429288ca84e11b592628e8e686a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8f8d8c41f64b60b2f0c61aa0d09cec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e06801e83bc8449e9d18ccb53bbc7a2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e34a8632030d4df9b3a312bcaae27cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e77a2b6150de45c7bff263a6e66f66e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_466486dea96a4cad9b13128ff8c07820",
      "placeholder": "​",
      "style": "IPY_MODEL_55d5b578ebfe4bbe97d4c7c15794bb89",
      "value": " 8.15k/8.15k [00:00&lt;00:00, 657kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
